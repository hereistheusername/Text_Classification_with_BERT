{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Aim\n",
    "In this notebook, I'll use a TransformerEncoderLayer to perform text classification task. For simplicity, I'll directly use the pre-trained tokenizer since this doesn't affect the classification very much."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "# !pip install datasets transformers evaluate torchtext==0.6"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.modules import TransformerEncoderLayer, TransformerEncoder\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import math\n",
    "from tempfile import TemporaryDirectory\n",
    "import os\n",
    "import random\n",
    "from typing import Tuple"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "TOKEN_LENGTH = 10\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "random.seed(42)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load the dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset emotion (/Users/xinglanl/.cache/huggingface/datasets/dair-ai___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n",
      "Found cached dataset emotion (/Users/xinglanl/.cache/huggingface/datasets/dair-ai___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n",
      "Found cached dataset emotion (/Users/xinglanl/.cache/huggingface/datasets/dair-ai___emotion/split/1.0.0/cca5efe2dfeb58c1d098e0f9eeb200e9927d889b5a03c67097275dfb5fe463bd)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of train: 16000, validation: 2000, test: 2000\n"
     ]
    }
   ],
   "source": [
    "train = load_dataset('dair-ai/emotion', 'split', split='train')\n",
    "valid = load_dataset('dair-ai/emotion', 'split', split='validation')\n",
    "test = load_dataset('dair-ai/emotion', 'split', split='test')\n",
    "print('size of train: {}, validation: {}, test: {}'.format(len(train), len(valid), len(test)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pre-process data with pre-trained model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "outputs": [],
   "source": [
    "# [p.values() for p in train.to_iterable_dataset()]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [
    {
     "data": {
      "text/plain": "Dataset({\n    features: ['text', 'label'],\n    num_rows: 16000\n})"
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer(train['text'][0])\n",
    "train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "outputs": [],
   "source": [
    "TEXT = data.Field(sequential=True, lower=True, tokenize=str.split, batch_first=True, fix_length=TOKEN_LENGTH)\n",
    "LABEL = data.LabelField(sequential=False, use_vocab=False)\n",
    "fields = [('text', TEXT), ('label', LABEL)]\n",
    "# examples = [data.Example.fromlist([k, v], fields) for k,v in zip(train['text'], train['label'])]\n",
    "# print(type(examples[0]))\n",
    "# print(examples[0].text)\n",
    "# print(examples[0].label)\n",
    "class DataFrameDataset(data.Dataset):\n",
    "\n",
    "    def __init__(self, df, text_field, label_field, is_test=False, **kwargs):\n",
    "        fields = [('text', text_field), ('label', label_field)]\n",
    "        examples = []\n",
    "        for i, row in df.iterrows():\n",
    "            label = row.label\n",
    "            text = row.text\n",
    "            examples.append(data.Example.fromlist([text, label], fields))\n",
    "\n",
    "        super().__init__(examples, fields, **kwargs)\n",
    "train_dataset = DataFrameDataset(train.to_pandas(), TEXT,LABEL)\n",
    "valid_dataset = DataFrameDataset(valid.to_pandas(), TEXT,LABEL)\n",
    "test_dataset = DataFrameDataset(test.to_pandas(), TEXT,LABEL)\n",
    "new_corpus = [e.text for e in train_dataset.examples]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([26, 10])"
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.build_vocab(new_corpus)\n",
    "LABEL.build_vocab(train['label'])\n",
    "r = TEXT.process(train['text'][1000].split())\n",
    "r.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of TEXT vocabulary: 15214\n",
      "Size of LABEL vocabulary: 6\n"
     ]
    }
   ],
   "source": [
    "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n",
    "print(\"Size of LABEL vocabulary:\",len(LABEL.vocab))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [
    {
     "data": {
      "text/plain": "[\n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 64]\n \t[.text]:[torch.LongTensor of size 64x10]\n \t[.label]:[torch.LongTensor of size 64],\n \n [torchtext.data.batch.Batch of size 16]\n \t[.text]:[torch.LongTensor of size 16x10]\n \t[.label]:[torch.LongTensor of size 16]]"
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_iterator,valid_iterator,test_iterator= data.BucketIterator.splits(\n",
    "    (train_dataset, valid_dataset,test_dataset),\n",
    "    batch_size = BATCH_SIZE,\n",
    "    sort =False,\n",
    "    shuffle=False)\n",
    "[ b for b in test_iterator]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "outputs": [],
   "source": [
    "# def tokenizing(record):\n",
    "#\n",
    "#     return tokenizer(record['text'], truncation=True, max_length=300)\n",
    "#\n",
    "#\n",
    "# train_tokenized = train.map(tokenizing, batched=True)\n",
    "# valid_tokenized = valid.map(tokenizing, batched=True)\n",
    "# test_tokenized = test.map(tokenizing, batched=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "outputs": [],
   "source": [
    "# def func(l):\n",
    "#     t = torch.Tensor(7)\n",
    "#     t[l] = 1\n",
    "#     return t\n",
    "#\n",
    "#\n",
    "# print(train_tokenized['label'][:2])\n",
    "# [func(l) for l in train_tokenized['label'][:2]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "# train_tokenized[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "outputs": [],
   "source": [
    "# id2label = {\n",
    "#     0: 'sadness',\n",
    "#     1: 'joy',\n",
    "#     2: 'love',\n",
    "#     3: 'anger',\n",
    "#     4: 'fear',\n",
    "#     5: 'surprise'\n",
    "# }\n",
    "# label2id = {v: k for k, v in id2label.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [],
   "source": [
    "accuracy = evaluate.load('accuracy')\n",
    "\n",
    "\n",
    "def metrics(pred, true):\n",
    "    predictions = np.argmax(pred, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=true)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [],
   "source": [
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, nhead: int, d_hid: int, nlayers: int, out_features: int,\n",
    "                in_between: int = 128, dropout: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.model_type = 'Transformer text classificator'\n",
    "        self.linear_in = TOKEN_LENGTH * d_model\n",
    "        self.encoder = nn.Embedding(len(TEXT.vocab), d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)\n",
    "        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)\n",
    "        self.linear1 = nn.modules.Linear(self.linear_in,   in_between)\n",
    "        self.output = nn.modules.Linear( in_between, out_features)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, src: Tensor, mask: Tensor, padding_masks: Tensor) -> Tensor:\n",
    "        src = self.encoder(src) * math.sqrt(self.d_model)\n",
    "        src = self.pos_encoder(src)\n",
    "        x = self.transformer_encoder(src, mask=mask, src_key_padding_mask=padding_masks)\n",
    "        x = x.reshape(-1, self.linear_in)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        output = self.output(x)\n",
    "        return output"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, 1, d_model)\n",
    "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:x.size(0)]\n",
    "        return self.dropout(x)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# training arguments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "data": {
      "text/plain": "TransformerModel(\n  (encoder): Embedding(15214, 300)\n  (pos_encoder): PositionalEncoding(\n    (dropout): Dropout(p=0, inplace=False)\n  )\n  (transformer_encoder): TransformerEncoder(\n    (layers): ModuleList(\n      (0-1): 2 x TransformerEncoderLayer(\n        (self_attn): MultiheadAttention(\n          (out_proj): NonDynamicallyQuantizableLinear(in_features=300, out_features=300, bias=True)\n        )\n        (linear1): Linear(in_features=300, out_features=200, bias=True)\n        (dropout): Dropout(p=0, inplace=False)\n        (linear2): Linear(in_features=200, out_features=300, bias=True)\n        (norm1): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n        (norm2): LayerNorm((300,), eps=1e-05, elementwise_affine=True)\n        (dropout1): Dropout(p=0, inplace=False)\n        (dropout2): Dropout(p=0, inplace=False)\n      )\n    )\n  )\n  (linear1): Linear(in_features=3000, out_features=128, bias=True)\n  (output): Linear(in_features=128, out_features=6, bias=True)\n)"
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emsize = 300  # embedding dimension\n",
    "d_hid = 200  # dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2  # number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "out_features = 6  # num of categories\n",
    "nhead = 2  # number of heads in nn.MultiheadAttention\n",
    "dropout = 0  # dropout probability\n",
    "model = TransformerModel(emsize, nhead, d_hid, nlayers, out_features, dropout=dropout).to(device)\n",
    "model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(sz: int) -> Tensor:\n",
    "    \"\"\"Generates an upper-triangular matrix of -inf, with zeros on diag.\"\"\"\n",
    "    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "def reshape_embeddings(dataset):\n",
    "    embeddings = [torch.tensor(l, dtype=torch.float) for l in dataset['input_ids']]\n",
    "    # embeddings.append(torch.zeros(emsize))    # add extract tensor to control the length of each text\n",
    "    # embeddings = pad_sequence(embeddings, batch_first=True)[:-1]    # remove the controal tensor\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "# def reshape_embeddings(dataset):\n",
    "#     for l in dataset['input_ids']:\n",
    "\n",
    "# train_embeddings = reshape_embeddings(train_tokenized)\n",
    "# valid_embeddings = reshape_embeddings(valid_tokenized)\n",
    "# test_embeddings = reshape_embeddings(test_tokenized)\n",
    "# train_labels = torch.tensor(train_tokenized['label'])\n",
    "# valid_labels = torch.tensor(valid_tokenized['label'])\n",
    "# test_labels = torch.tensor(test_tokenized['label'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 1e-3  # learning rate\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=lr)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "\n",
    "def train(model: nn.Module, epoch: int, dataset: data.Iterator) -> None:\n",
    "    model.train()  # turn on train mode\n",
    "    # total_loss = 0.\n",
    "    # log_interval = 10\n",
    "    # start_time = time.time()\n",
    "    # src_mask = generate_square_subsequent_mask(BATCH_SIZE).to(device)\n",
    "\n",
    "    for batch in dataset:\n",
    "        texts = batch.text\n",
    "        labels = batch.label\n",
    "        outputs = model(texts.to(device), None, None)\n",
    "        loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        if dataset.iterations % 100 == 0:\n",
    "            loss, current = loss.item(), (dataset.iterations + 1) * len(batch)\n",
    "            # print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            print('loss: {:.7f} progress: [{:>5d}/{:>5d}]'.format(loss, current, len(dataset.dataset)) )\n",
    "        # total_loss += loss.item()\n",
    "        # if dataset.iterations % log_interval == 0 and dataset.iterations > 0:\n",
    "        #     # lr = scheduler.get_last_lr()[0]\n",
    "        #     ms_per_batch = (time.time() - start_time) * 1000 / log_interval\n",
    "        #     cur_loss = total_loss / log_interval\n",
    "        #     ppl = math.exp(cur_loss)\n",
    "        #     print(f'| epoch {epoch:3d} | {(dataset.iterations):5d}/{len(dataset):5d} batches | '\n",
    "        #           f'lr {lr:04.4f} | ms/batch{ms_per_batch:5.2f} | '\n",
    "        #           f'loss {cur_loss:5.2f} | ppl {ppl:8.2f}')\n",
    "        #     total_loss = 0\n",
    "        #     start_time = time.time()\n",
    "\n",
    "\n",
    "def evaluate(model: nn.Module, dataset: data.Iterator) -> Tuple[float, float, list, list]:\n",
    "    model.eval()  # turn on evaluation mode\n",
    "    total_loss, acc = 0.0, 0.0\n",
    "    pred_y_arr = list()\n",
    "    comparison_arr = list()\n",
    "    src_mask = generate_square_subsequent_mask(BATCH_SIZE).to(device)\n",
    "    with torch.no_grad():\n",
    "        for i,batch in enumerate(dataset):\n",
    "            texts = (batch.text).to(device)\n",
    "            labels = (batch.label).to(device)\n",
    "            # outputs = model(texts.to(device))\n",
    "            outputs = model(texts, None, None)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            pred_y = outputs.argmax(1)\n",
    "            pred_y_arr.append(pred_y)\n",
    "            compare = (pred_y == labels).type(torch.float)\n",
    "            comparison_arr.append(compare)\n",
    "            acc += compare.sum().item()\n",
    "\n",
    "        total_loss /= (i+1)\n",
    "        acc /= len(dataset.dataset)\n",
    "        print('Evaluate: avg loss: {:8>f} , accuracy: {:0.1f}'.format(total_loss, 100*acc))\n",
    "    return total_loss, acc, pred_y_arr, comparison_arr\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------epoch: 1------------------------------\n",
      "loss: 0.9334722 progress: [ 6464/16000]\n",
      "loss: 0.9806864 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.479408 , accuracy: 47.1\n",
      "epoch elapsed time: 5.700689315795898\n",
      "------------------------------epoch: 2------------------------------\n",
      "loss: 0.7278088 progress: [ 6464/16000]\n",
      "loss: 0.8074391 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.506570 , accuracy: 48.4\n",
      "epoch elapsed time: 5.538240194320679\n",
      "------------------------------epoch: 3------------------------------\n",
      "loss: 0.6265860 progress: [ 6464/16000]\n",
      "loss: 0.6763330 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.536326 , accuracy: 49.1\n",
      "epoch elapsed time: 5.591440916061401\n",
      "------------------------------epoch: 4------------------------------\n",
      "loss: 0.5430669 progress: [ 6464/16000]\n",
      "loss: 0.5861725 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.570173 , accuracy: 49.5\n",
      "epoch elapsed time: 5.604636907577515\n",
      "------------------------------epoch: 5------------------------------\n",
      "loss: 0.4765886 progress: [ 6464/16000]\n",
      "loss: 0.4837693 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.613536 , accuracy: 48.9\n",
      "epoch elapsed time: 5.6785149574279785\n",
      "------------------------------epoch: 6------------------------------\n",
      "loss: 0.4193627 progress: [ 6464/16000]\n",
      "loss: 0.4295852 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.648697 , accuracy: 49.6\n",
      "epoch elapsed time: 5.677248001098633\n",
      "------------------------------epoch: 7------------------------------\n",
      "loss: 0.3749328 progress: [ 6464/16000]\n",
      "loss: 0.3715305 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.696061 , accuracy: 49.7\n",
      "epoch elapsed time: 5.605562210083008\n",
      "------------------------------epoch: 8------------------------------\n",
      "loss: 0.3252676 progress: [ 6464/16000]\n",
      "loss: 0.3208783 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.730457 , accuracy: 50.0\n",
      "epoch elapsed time: 5.643399000167847\n",
      "------------------------------epoch: 9------------------------------\n",
      "loss: 0.2841361 progress: [ 6464/16000]\n",
      "loss: 0.2865323 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.773556 , accuracy: 49.9\n",
      "epoch elapsed time: 5.695251941680908\n",
      "------------------------------epoch: 10------------------------------\n",
      "loss: 0.2584414 progress: [ 6464/16000]\n",
      "loss: 0.2480221 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.811261 , accuracy: 50.0\n",
      "epoch elapsed time: 5.658686876296997\n",
      "------------------------------epoch: 11------------------------------\n",
      "loss: 0.2301751 progress: [ 6464/16000]\n",
      "loss: 0.2191764 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.844365 , accuracy: 50.0\n",
      "epoch elapsed time: 5.65166974067688\n",
      "------------------------------epoch: 12------------------------------\n",
      "loss: 0.2074304 progress: [ 6464/16000]\n",
      "loss: 0.1927576 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.884786 , accuracy: 50.2\n",
      "epoch elapsed time: 5.691758871078491\n",
      "------------------------------epoch: 13------------------------------\n",
      "loss: 0.1885083 progress: [ 6464/16000]\n",
      "loss: 0.1721662 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.929420 , accuracy: 50.0\n",
      "epoch elapsed time: 5.670870065689087\n",
      "------------------------------epoch: 14------------------------------\n",
      "loss: 0.1667743 progress: [ 6464/16000]\n",
      "loss: 0.1538243 progress: [12864/16000]\n",
      "Evaluate: avg loss: 1.969376 , accuracy: 50.2\n",
      "epoch elapsed time: 5.566481113433838\n",
      "------------------------------epoch: 15------------------------------\n",
      "loss: 0.1515112 progress: [ 6464/16000]\n",
      "loss: 0.1390675 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.006746 , accuracy: 50.2\n",
      "epoch elapsed time: 5.667550086975098\n",
      "------------------------------epoch: 16------------------------------\n",
      "loss: 0.1342737 progress: [ 6464/16000]\n",
      "loss: 0.1252422 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.039253 , accuracy: 50.1\n",
      "epoch elapsed time: 5.659215927124023\n",
      "------------------------------epoch: 17------------------------------\n",
      "loss: 0.1219464 progress: [ 6464/16000]\n",
      "loss: 0.1130824 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.079931 , accuracy: 50.0\n",
      "epoch elapsed time: 5.700473070144653\n",
      "------------------------------epoch: 18------------------------------\n",
      "loss: 0.1106952 progress: [ 6464/16000]\n",
      "loss: 0.1025874 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.115425 , accuracy: 49.9\n",
      "epoch elapsed time: 5.64480185508728\n",
      "------------------------------epoch: 19------------------------------\n",
      "loss: 0.1030664 progress: [ 6464/16000]\n",
      "loss: 0.0943007 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.152862 , accuracy: 49.8\n",
      "epoch elapsed time: 5.649497985839844\n",
      "------------------------------epoch: 20------------------------------\n",
      "loss: 0.0931339 progress: [ 6464/16000]\n",
      "loss: 0.0863165 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.194093 , accuracy: 49.9\n",
      "epoch elapsed time: 5.763597726821899\n",
      "------------------------------epoch: 21------------------------------\n",
      "loss: 0.0872253 progress: [ 6464/16000]\n",
      "loss: 0.0785647 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.228875 , accuracy: 49.6\n",
      "epoch elapsed time: 5.676677942276001\n",
      "------------------------------epoch: 22------------------------------\n",
      "loss: 0.0789137 progress: [ 6464/16000]\n",
      "loss: 0.0722030 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.260536 , accuracy: 49.3\n",
      "epoch elapsed time: 5.612507343292236\n",
      "------------------------------epoch: 23------------------------------\n",
      "loss: 0.0731201 progress: [ 6464/16000]\n",
      "loss: 0.0649752 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.298565 , accuracy: 49.2\n",
      "epoch elapsed time: 5.625340938568115\n",
      "------------------------------epoch: 24------------------------------\n",
      "loss: 0.0675299 progress: [ 6464/16000]\n",
      "loss: 0.0600746 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.330862 , accuracy: 49.5\n",
      "epoch elapsed time: 5.687772989273071\n",
      "------------------------------epoch: 25------------------------------\n",
      "loss: 0.0624984 progress: [ 6464/16000]\n",
      "loss: 0.0559997 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.364110 , accuracy: 49.5\n",
      "epoch elapsed time: 5.7538628578186035\n",
      "------------------------------epoch: 26------------------------------\n",
      "loss: 0.0593533 progress: [ 6464/16000]\n",
      "loss: 0.0520062 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.401961 , accuracy: 49.5\n",
      "epoch elapsed time: 5.615066051483154\n",
      "------------------------------epoch: 27------------------------------\n",
      "loss: 0.0553814 progress: [ 6464/16000]\n",
      "loss: 0.0485395 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.433232 , accuracy: 49.5\n",
      "epoch elapsed time: 6.140321969985962\n",
      "------------------------------epoch: 28------------------------------\n",
      "loss: 0.0507591 progress: [ 6464/16000]\n",
      "loss: 0.0452502 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.465042 , accuracy: 49.5\n",
      "epoch elapsed time: 5.65383505821228\n",
      "------------------------------epoch: 29------------------------------\n",
      "loss: 0.0475310 progress: [ 6464/16000]\n",
      "loss: 0.0424143 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.497133 , accuracy: 49.4\n",
      "epoch elapsed time: 5.662514925003052\n",
      "------------------------------epoch: 30------------------------------\n",
      "loss: 0.0446677 progress: [ 6464/16000]\n",
      "loss: 0.0398694 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.522274 , accuracy: 49.5\n",
      "epoch elapsed time: 5.883790969848633\n",
      "------------------------------epoch: 31------------------------------\n",
      "loss: 0.0420971 progress: [ 6464/16000]\n",
      "loss: 0.0374946 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.550898 , accuracy: 49.5\n",
      "epoch elapsed time: 5.687645196914673\n",
      "------------------------------epoch: 32------------------------------\n",
      "loss: 0.0397136 progress: [ 6464/16000]\n",
      "loss: 0.0353155 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.577531 , accuracy: 49.5\n",
      "epoch elapsed time: 5.607789993286133\n",
      "------------------------------epoch: 33------------------------------\n",
      "loss: 0.0374893 progress: [ 6464/16000]\n",
      "loss: 0.0333007 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.604645 , accuracy: 49.6\n",
      "epoch elapsed time: 5.754019260406494\n",
      "------------------------------epoch: 34------------------------------\n",
      "loss: 0.0354586 progress: [ 6464/16000]\n",
      "loss: 0.0314511 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.631266 , accuracy: 49.6\n",
      "epoch elapsed time: 5.638628721237183\n",
      "------------------------------epoch: 35------------------------------\n",
      "loss: 0.0336535 progress: [ 6464/16000]\n",
      "loss: 0.0298107 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.656338 , accuracy: 49.7\n",
      "epoch elapsed time: 5.725770950317383\n",
      "------------------------------epoch: 36------------------------------\n",
      "loss: 0.0319718 progress: [ 6464/16000]\n",
      "loss: 0.0282571 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.681899 , accuracy: 49.8\n",
      "epoch elapsed time: 5.6257898807525635\n",
      "------------------------------epoch: 37------------------------------\n",
      "loss: 0.0303983 progress: [ 6464/16000]\n",
      "loss: 0.0268893 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.705911 , accuracy: 49.9\n",
      "epoch elapsed time: 5.81666898727417\n",
      "------------------------------epoch: 38------------------------------\n",
      "loss: 0.0288808 progress: [ 6464/16000]\n",
      "loss: 0.0255525 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.727708 , accuracy: 49.9\n",
      "epoch elapsed time: 5.529820919036865\n",
      "------------------------------epoch: 39------------------------------\n",
      "loss: 0.0272592 progress: [ 6464/16000]\n",
      "loss: 0.0243305 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.750499 , accuracy: 49.9\n",
      "epoch elapsed time: 5.460485219955444\n",
      "------------------------------epoch: 40------------------------------\n",
      "loss: 0.0260155 progress: [ 6464/16000]\n",
      "loss: 0.0232426 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.773006 , accuracy: 49.9\n",
      "epoch elapsed time: 5.471979856491089\n",
      "------------------------------epoch: 41------------------------------\n",
      "loss: 0.0248439 progress: [ 6464/16000]\n",
      "loss: 0.0221403 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.792578 , accuracy: 49.9\n",
      "epoch elapsed time: 5.6872100830078125\n",
      "------------------------------epoch: 42------------------------------\n",
      "loss: 0.0237890 progress: [ 6464/16000]\n",
      "loss: 0.0209635 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.815246 , accuracy: 50.0\n",
      "epoch elapsed time: 5.4775660037994385\n",
      "------------------------------epoch: 43------------------------------\n",
      "loss: 0.0228008 progress: [ 6464/16000]\n",
      "loss: 0.0193812 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.835225 , accuracy: 50.0\n",
      "epoch elapsed time: 5.488785028457642\n",
      "------------------------------epoch: 44------------------------------\n",
      "loss: 0.0218315 progress: [ 6464/16000]\n",
      "loss: 0.0185424 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.857512 , accuracy: 50.0\n",
      "epoch elapsed time: 5.4251179695129395\n",
      "------------------------------epoch: 45------------------------------\n",
      "loss: 0.0209646 progress: [ 6464/16000]\n",
      "loss: 0.0177569 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.875796 , accuracy: 50.0\n",
      "epoch elapsed time: 5.577840328216553\n",
      "------------------------------epoch: 46------------------------------\n",
      "loss: 0.0201361 progress: [ 6464/16000]\n",
      "loss: 0.0170483 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.894935 , accuracy: 50.0\n",
      "epoch elapsed time: 5.556783199310303\n",
      "------------------------------epoch: 47------------------------------\n",
      "loss: 0.0193537 progress: [ 6464/16000]\n",
      "loss: 0.0163936 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.913471 , accuracy: 50.0\n",
      "epoch elapsed time: 5.524476051330566\n",
      "------------------------------epoch: 48------------------------------\n",
      "loss: 0.0186484 progress: [ 6464/16000]\n",
      "loss: 0.0157796 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.931834 , accuracy: 50.0\n",
      "epoch elapsed time: 5.532174825668335\n",
      "------------------------------epoch: 49------------------------------\n",
      "loss: 0.0179425 progress: [ 6464/16000]\n",
      "loss: 0.0152136 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.950118 , accuracy: 50.0\n",
      "epoch elapsed time: 5.429543972015381\n",
      "------------------------------epoch: 50------------------------------\n",
      "loss: 0.0172982 progress: [ 6464/16000]\n",
      "loss: 0.0146742 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.968342 , accuracy: 50.0\n",
      "epoch elapsed time: 5.455486059188843\n",
      "------------------------------epoch: 51------------------------------\n",
      "loss: 0.0166967 progress: [ 6464/16000]\n",
      "loss: 0.0141839 progress: [12864/16000]\n",
      "Evaluate: avg loss: 2.985859 , accuracy: 50.0\n",
      "epoch elapsed time: 5.449104070663452\n",
      "------------------------------epoch: 52------------------------------\n",
      "loss: 0.0161257 progress: [ 6464/16000]\n",
      "loss: 0.0137042 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.002783 , accuracy: 50.0\n",
      "epoch elapsed time: 5.465177059173584\n",
      "------------------------------epoch: 53------------------------------\n",
      "loss: 0.0155855 progress: [ 6464/16000]\n",
      "loss: 0.0132586 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.020286 , accuracy: 50.1\n",
      "epoch elapsed time: 5.476410865783691\n",
      "------------------------------epoch: 54------------------------------\n",
      "loss: 0.0150711 progress: [ 6464/16000]\n",
      "loss: 0.0128339 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.036736 , accuracy: 50.2\n",
      "epoch elapsed time: 5.4732160568237305\n",
      "------------------------------epoch: 55------------------------------\n",
      "loss: 0.0145810 progress: [ 6464/16000]\n",
      "loss: 0.0124384 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.053258 , accuracy: 50.2\n",
      "epoch elapsed time: 5.4513208866119385\n",
      "------------------------------epoch: 56------------------------------\n",
      "loss: 0.0141124 progress: [ 6464/16000]\n",
      "loss: 0.0120531 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.069599 , accuracy: 50.3\n",
      "epoch elapsed time: 5.573966026306152\n",
      "------------------------------epoch: 57------------------------------\n",
      "loss: 0.0136624 progress: [ 6464/16000]\n",
      "loss: 0.0116881 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.085258 , accuracy: 50.3\n",
      "epoch elapsed time: 5.475711107254028\n",
      "------------------------------epoch: 58------------------------------\n",
      "loss: 0.0132332 progress: [ 6464/16000]\n",
      "loss: 0.0113360 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.100267 , accuracy: 50.3\n",
      "epoch elapsed time: 5.554577112197876\n",
      "------------------------------epoch: 59------------------------------\n",
      "loss: 0.0128398 progress: [ 6464/16000]\n",
      "loss: 0.0110112 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.116093 , accuracy: 50.4\n",
      "epoch elapsed time: 5.4972310066223145\n",
      "------------------------------epoch: 60------------------------------\n",
      "loss: 0.0124590 progress: [ 6464/16000]\n",
      "loss: 0.0106966 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.130635 , accuracy: 50.4\n",
      "epoch elapsed time: 5.455876111984253\n",
      "------------------------------epoch: 61------------------------------\n",
      "loss: 0.0121000 progress: [ 6464/16000]\n",
      "loss: 0.0104136 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.144929 , accuracy: 50.4\n",
      "epoch elapsed time: 5.441702842712402\n",
      "------------------------------epoch: 62------------------------------\n",
      "loss: 0.0117547 progress: [ 6464/16000]\n",
      "loss: 0.0101219 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.159712 , accuracy: 50.3\n",
      "epoch elapsed time: 5.5365917682647705\n",
      "------------------------------epoch: 63------------------------------\n",
      "loss: 0.0114308 progress: [ 6464/16000]\n",
      "loss: 0.0098594 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.173645 , accuracy: 50.3\n",
      "epoch elapsed time: 5.551644802093506\n",
      "------------------------------epoch: 64------------------------------\n",
      "loss: 0.0111155 progress: [ 6464/16000]\n",
      "loss: 0.0096011 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.188216 , accuracy: 50.2\n",
      "epoch elapsed time: 5.500672101974487\n",
      "------------------------------epoch: 65------------------------------\n",
      "loss: 0.0108215 progress: [ 6464/16000]\n",
      "loss: 0.0093519 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.201328 , accuracy: 50.3\n",
      "epoch elapsed time: 5.534342288970947\n",
      "------------------------------epoch: 66------------------------------\n",
      "loss: 0.0105302 progress: [ 6464/16000]\n",
      "loss: 0.0091215 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.214930 , accuracy: 50.3\n",
      "epoch elapsed time: 6.066169738769531\n",
      "------------------------------epoch: 67------------------------------\n",
      "loss: 0.0102484 progress: [ 6464/16000]\n",
      "loss: 0.0088962 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.227692 , accuracy: 50.2\n",
      "epoch elapsed time: 5.700451135635376\n",
      "------------------------------epoch: 68------------------------------\n",
      "loss: 0.0099902 progress: [ 6464/16000]\n",
      "loss: 0.0086841 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.241272 , accuracy: 50.3\n",
      "epoch elapsed time: 5.791079998016357\n",
      "------------------------------epoch: 69------------------------------\n",
      "loss: 0.0097305 progress: [ 6464/16000]\n",
      "loss: 0.0084755 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.253093 , accuracy: 50.3\n",
      "epoch elapsed time: 5.598696947097778\n",
      "------------------------------epoch: 70------------------------------\n",
      "loss: 0.0094841 progress: [ 6464/16000]\n",
      "loss: 0.0082796 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.266511 , accuracy: 50.3\n",
      "epoch elapsed time: 5.769959926605225\n",
      "------------------------------epoch: 71------------------------------\n",
      "loss: 0.0092533 progress: [ 6464/16000]\n",
      "loss: 0.0080882 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.278584 , accuracy: 50.5\n",
      "epoch elapsed time: 5.728292226791382\n",
      "------------------------------epoch: 72------------------------------\n",
      "loss: 0.0090247 progress: [ 6464/16000]\n",
      "loss: 0.0079062 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.291449 , accuracy: 50.5\n",
      "epoch elapsed time: 5.735850095748901\n",
      "------------------------------epoch: 73------------------------------\n",
      "loss: 0.0088115 progress: [ 6464/16000]\n",
      "loss: 0.0077338 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.302733 , accuracy: 50.6\n",
      "epoch elapsed time: 5.650045156478882\n",
      "------------------------------epoch: 74------------------------------\n",
      "loss: 0.0086010 progress: [ 6464/16000]\n",
      "loss: 0.0075643 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.315502 , accuracy: 50.5\n",
      "epoch elapsed time: 5.65125298500061\n",
      "------------------------------epoch: 75------------------------------\n",
      "loss: 0.0084057 progress: [ 6464/16000]\n",
      "loss: 0.0074048 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.326718 , accuracy: 50.5\n",
      "epoch elapsed time: 5.633603096008301\n",
      "------------------------------epoch: 76------------------------------\n",
      "loss: 0.0082123 progress: [ 6464/16000]\n",
      "loss: 0.0072462 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.338684 , accuracy: 50.4\n",
      "epoch elapsed time: 5.52811074256897\n",
      "------------------------------epoch: 77------------------------------\n",
      "loss: 0.0080267 progress: [ 6464/16000]\n",
      "loss: 0.0070954 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.350374 , accuracy: 50.5\n",
      "epoch elapsed time: 5.675873041152954\n",
      "------------------------------epoch: 78------------------------------\n",
      "loss: 0.0078481 progress: [ 6464/16000]\n",
      "loss: 0.0069493 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.361925 , accuracy: 50.4\n",
      "epoch elapsed time: 5.6358067989349365\n",
      "------------------------------epoch: 79------------------------------\n",
      "loss: 0.0076797 progress: [ 6464/16000]\n",
      "loss: 0.0068099 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.372972 , accuracy: 50.5\n",
      "epoch elapsed time: 5.647566080093384\n",
      "------------------------------epoch: 80------------------------------\n",
      "loss: 0.0075111 progress: [ 6464/16000]\n",
      "loss: 0.0066751 progress: [12864/16000]\n",
      "Evaluate: avg loss: 3.384348 , accuracy: 50.4\n",
      "epoch elapsed time: 5.746835231781006\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "epochs = 80\n",
    "\n",
    "with TemporaryDirectory() as tempdir:\n",
    "    best_model_params_path = os.path.join(tempdir, \"best_model_params.pt\")\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        print('-'*30 + 'epoch: {}'.format(epoch) + '-'*30 )\n",
    "        epoch_start_time = time.time()\n",
    "        train(model, epoch, train_iterator)\n",
    "        val_loss, *_ = evaluate(model, valid_iterator)\n",
    "\n",
    "        elapsed = time.time() - epoch_start_time\n",
    "        print('epoch elapsed time: {}'.format(elapsed))\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), best_model_params_path)\n",
    "\n",
    "        # scheduler.step()\n",
    "    model.load_state_dict(torch.load(best_model_params_path))  # load best model states"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluate: avg loss: 1.421456 , accuracy: 49.1\n"
     ]
    },
    {
     "data": {
      "text/plain": "tensor([0., 0., 0., 0., 1., 0., 1., 1., 1., 0., 0., 0., 0., 1., 0., 1., 1., 0.,\n        0., 0., 1., 0., 1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 1., 1., 0., 1.,\n        1., 1., 0., 0., 1., 1., 1., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 1.,\n        0., 0., 0., 1., 1., 0., 1., 1., 0., 0.])"
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "*_, comp = evaluate(model, test_iterator)\n",
    "comp[0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [],
   "source": [
    "torch.save(model, 'output/encoder-03-15')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
